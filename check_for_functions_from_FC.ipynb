{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy_from_webp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's to check first class from copy of web classes\n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(Conv2D.apply, (X, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unsqueeze_all(t):\n",
    "    # Helper function to ``unsqueeze`` all the dimensions that we reduce over\n",
    "    return t[None, :, None, None]\n",
    "def batch_norm_backward(grad_out, X, sum, sqrt_var, N, eps):\n",
    "    # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``\n",
    "    # in batch norm 2D forward. To simplify our derivation, we follow the\n",
    "    # chain rule and compute the gradients as follows before accumulating\n",
    "    # them all into a final grad_input.\n",
    "    #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``\n",
    "    #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``\n",
    "    #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``\n",
    "    # We then rewrite the formulas to use as few extra buffers as possible\n",
    "    tmp = ((X - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n",
    "    tmp *= -1\n",
    "    d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``\n",
    "    # It is useful to delete tensors when you no longer need them with ``del``\n",
    "    # For example, we could've done ``del tmp`` here because we won't use it later\n",
    "    # In this case, it's not a big difference because ``tmp`` only has size of (C,)\n",
    "    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n",
    "    d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``\n",
    "    # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer\n",
    "    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n",
    "    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n",
    "    # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry\n",
    "\n",
    "    # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``\n",
    "    grad_input = X * unsqueeze_all(d_var * N)\n",
    "    grad_input += unsqueeze_all(-d_var * sum)\n",
    "    grad_input *= 2 / ((N - 1) * N)\n",
    "    # (2) mean (see above)\n",
    "    grad_input += d_mean_dx\n",
    "    # (3) Add 'grad_out / <factor>' without allocating an extra buffer\n",
    "    grad_input *= unsqueeze_all(sqrt_var + eps)\n",
    "    grad_input += grad_out\n",
    "    grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``\n",
    "    return grad_input\n",
    "class BatchNorm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, eps=1e-3):\n",
    "        # Don't save ``keepdim`` values for backward\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.save_for_backward(X)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, = ctx.saved_tensors\n",
    "        return batch_norm_backward(grad_out, X, ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)\n",
    "a = torch.rand(1, 2, 3, 4, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(BatchNorm.apply, (a,), fast_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
